---
title: "Target Markdown"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

stuff to test:

-   

[TARGETS theory]{.ul}

(didnt add any notes from each chapter, check them later to see if you get any useful additional info).

------------------------------------------------------------------------

from <https://github.com/jdblischak/workflowr/issues/238>

wlandau writes:

I just added some clarifying comments to the manual. You should be able to spread your work over multiple reports as long as all the code chunks have unique labels. The [`_targets.R` file for Target Markdown](https://github.com/ropensci/targets/blob/main/inst/pipelines/_targets_r.R) is always the same no matter what code chunk writes it, and the file only actually gets written if the hash is different from that of the template. Here is what the file looks like.

\
There is nothing specific to any particular chunk. The `_targets.R` file simply reads the independent chunk-specific scripts written to the `_targets_r/` folder. That way, you can have as many reports as you want as long as the chunk labels do not collide

------------------------------------------------------------------------

1\. INTRODUCTION

The [`targets`](https://docs.ropensci.org/targets/) package is a [Make](https://www.gnu.org/software/make/)-like pipeline toolkit for Statistics and data science in R. With [`targets`](https://docs.ropensci.org/targets/), you can maintain a reproducible workflow without repeating yourself. [`targets`](https://docs.ropensci.org/targets/) learns how your pipeline fits together, skips costly runtime for tasks that are already up to date, runs only the necessary computation, supports implicit parallel computing, abstracts files as R objects, <!--# maybe add what comes next to the paper. -->and shows tangible evidence that the results match the underlying code and data.

[Pipeline toolkits](https://github.com/pditommaso/awesome-pipeline) like [GNU Make](https://www.gnu.org/software/make/) break the cycle. They watch the dependency graph of the whole workflow and skip steps, or "targets", whose code, data, and upstream dependencies have not changed since the last run of the pipeline. When all targets are up to date, this is evidence that the results match the underlying code and data, which helps us trust the results and confirm the computation is reproducible.

------------------------------------------------------------------------

2\. WALKTHROUGH

The file structure of the project looks like this.

    ├── _targets.R
    ├── R/
    ├──── functions.R
    ├── data/
    └──── raw_data.csv

`functions.R` contains our custom user-defined functions. (See the [functions chapter](https://books.ropensci.org/targets/functions.html#functions) for a discussion of function-oriented workflows.)

    # functions.R
    create_plot <- function(data) {
      ggplot(data) +
        geom_histogram(aes(x = Ozone)) +
        theme_gray(24)
    }

Whereas files `raw_data.csv` and `functions.R` are typical user-defined components of a [project-oriented workflow](https://rstats.wtf/project-oriented-workflow.html), the target script file `_targets.R` file is special. Every [`targets`](https://github.com/ropensci/targets) workflow needs a [target script file](https://docs.ropensci.org/targets/reference/tar_script.html) to formally define the targets in the pipeline. By default, the target script is a file called `_targets.R` in the project's [root directory](https://martinctc.github.io/blog/rstudio-projects-and-working-directories-a-beginner%27s-guide/).^[1](https://books.ropensci.org/targets/walkthrough.html#fn1)^ Functions [`tar_script()`](https://docs.ropensci.org/targets/reference/tar_script.html) and [`tar_edit()`](https://docs.ropensci.org/targets/reference/tar_edit.html) can help you create a target script file. Ours looks like this:

    # _targets.R file
    library(targets)
    source("R/functions.R")
    options(tidyverse.quiet = TRUE)
    tar_option_set(packages = c("biglm", "tidyverse"))
    list(
      tar_target(
        raw_data_file,
        "data/raw_data.csv",
        format = "file"
      ),
      tar_target(
        raw_data,
        read_csv(raw_data_file, col_types = cols())
      ),
      tar_target(
        data,
        raw_data %>%
          filter(!is.na(Ozone))
      ),
      tar_target(hist, create_plot(data)),
      tar_target(fit, biglm(Ozone ~ Wind + Temp, data))
    )

All target script files have these requirements.

1.  Load the [`targets`](https://github.com/ropensci/targets) package itself. (target scripts created with `tar_script()` automatically insert a `library(targets)` line at the top by default.)

2.  Load your custom functions and global objects into the R session. In our case, our only such object is the `create_plot()` function, and we load it into the session by calling `source("R/functions.R")`.

3.  Call `tar_option_set()` to set the default settings for all you targets, such as the names of required packages and the data storage format. Individual targets can override these settings.

4.  Define individual targets with the `tar_target()` function. Each target is an intermediate step of the workflow. At minimum, a target must have a name and an R expression. This expression runs when the pipeline builds the target, and the return value is saved as a file in the `_targets/objects/` folder. The only targets not stored in `_targets/objects/` are dynamic files such as `raw_data_file`. Here, `format = "file"` makes `raw_data_file` a dynamic file. That means `targets` watches the data at the file paths returned from the expression (in this case, `"data/raw_data.csv")`.^[2](https://books.ropensci.org/targets/walkthrough.html#fn2)^

5.  Every target script must end with a list of your `tar_target()` objects. Those objects can be nested, i.e. lists within lists.

Before you run the pipeline for real, you should always inspect the manifest and the graph for errors. `tar_manifest()` shows you a data frame information about the targets, and it has functionality to specify the targets and columns returned.

    tar_manifest(fields = "command")
    #> # A tibble: 5 × 2
    #>   name          command                                      
    #>   <chr>         <chr>                                        
    #> 1 raw_data_file "\"data/raw_data.csv\""                      
    #> 2 raw_data      "read_csv(raw_data_file, col_types = cols())"
    #> 3 data          "raw_data %>% filter(!is.na(Ozone))"         
    #> 4 fit           "biglm(Ozone ~ Wind + Temp, data)"           
    #> 5 hist          "create_plot(data)"

There are also graphical displays with `tar_glimpse()`

    tar_glimpse()

and `tar_visnetwork()`.

    tar_visnetwork()

Both graphing functions above visualize the underlying directed acyclic graph (DAG) and tell you how targets are connected. This DAG is indifferent to the order of targets in your pipeline. You will still get the same graph even if you rearrange them. This is because `targets` uses static code analysis to detect the dependencies of each target, and this process does not depend on target order. For details, visit the dependency detection section of the [target construction chapter](https://books.ropensci.org/targets/targets.html#targets).

`tar_make()` runs the workflow. It creates a fresh clean external R process, reads the target script to learn about the pipeline, runs the correct targets in the correct order given by the graph, and saves the necessary data to the `_targets/` data store. ^[3](https://books.ropensci.org/targets/walkthrough.html#fn3)^

    tar_make()
    #> • start target raw_data_file
    #> • built target raw_data_file
    #> • start target raw_data
    #> • built target raw_data
    #> • start target data
    #> • built target data
    #> • start target fit
    #> • built target fit
    #> • start target hist
    #> • built target hist
    #> • end pipeline

The next time you run `tar_make()`, `targets` skips everything that is already up to date, which saves a lot of time in large projects with long runtimes.

    tar_make()
    #> ✔ skip target raw_data_file
    #> ✔ skip target raw_data
    #> ✔ skip target data
    #> ✔ skip target fit
    #> ✔ skip target hist
    #> ✔ skip pipeline

You can use `tar_visnetwork()` and `tar_outdated()` to check ahead of time which targets are up to date.

The `targets` package notices when you make changes to code and data, and those changes affect which targets rerun and which targets are skipped. Internally, special rules called "cues" decide whether a target reruns. The [`tar_cue()`](https://docs.ropensci.org/targets/reference/tar_cue.html) function lets you suppress some of these cues, and the [`tarchetypes`](https://docs.ropensci.org/tarchetypes/) package supports nuanced [cue factories](https://docs.ropensci.org/tarchetypes/reference/index.html#section-cues) and [target factories](https://docs.ropensci.org/tarchetypes/reference/index.html#section-targets-with-custom-invalidation-rules) to further customize target invalidation behavior. The [`tar_cue()`](https://docs.ropensci.org/targets/reference/tar_cue.html) function documentation [explains cues in detail](https://docs.ropensci.org/targets/reference/tar_cue.html#target-invalidation-rules), as well as [specifics on how `targets` detects changes to upstream dependencies](https://docs.ropensci.org/targets/reference/tar_cue.html#dependency-based-invalidation-and-user-defined-functions).

**START** tar_cue() documentation

## **Target invalidation rules**

`targets` uses internal metadata and special cues to decide whether a target is up to date (can skip) or is outdated/invalidated (needs to rerun). By default, `targets` moves through the following list of cues and declares a target outdated if at least one is cue activated.

1.  There is no metadata record of the target.

2.  The target errored last run.

3.  The target has a different class than it did before.

4.  The cue mode equals `"always"`.

5.  The cue mode does not equal `"never"`.

6.  The `command` metadata field (the hash of the R command) is different from last time.

7.  The `depend` metadata field (the hash of the immediate upstream dependency targets and global objects) is different from last time.

8.  The storage format is different from last time.

9.  The iteration mode is different from last time.

10. A target's file (either the one in `_targets/objects/` or a dynamic file) does not exist or changed since last time.

The user can suppress many of the above cues using the `tar_cue()` function, which creates the `cue` argument of [`tar_target()`](https://docs.ropensci.org/targets/reference/tar_target.html). Cues objects also constitute more nuanced target invalidation rules. The `tarchetypes` package has many such examples, including `tar_age()`, `tar_download()`, `tar_cue_age()`, `tar_cue_force()`, and `tar_cue_skip()`.

## 

\
**Dependency-based invalidation and user-defined functions**

If the cue of a target has `depend = TRUE` (default) then the target is marked invalidated/outdated when its upstream dependencies change. A target's dependencies include upstream targets, user-defined functions, and other global objects populated in the target script file (default: `_targets.R`). To determine if a given dependency changed since the last run of the pipeline, `targets` computes hashes. The hash of a target is computed on its files in storage (usually a file in `_targets/objects/`). The hash of a non-function global object dependency is computed directly on its in-memory data. User-defined functions are hashed in the following way:

1.  Deparse the function with `targets:::tar_deparse_safe()`. This function computes a string representation of the function body and arguments. This string representation is invariant to changes in comments and whitespace, which means trivial changes to formatting do not cue targets to rerun.

2.  Manually remove any literal pointers from the function string using `targets:::mask_pointers()`. Such pointers arise from inline compiled C/C++ functions.

3.  Using static code analysis (i.e. [`tar_deps()`](https://docs.ropensci.org/targets/reference/tar_deps.html), which is based on [`codetools::findGlobals()`](https://rdrr.io/pkg/codetools/man/findGlobals.html)) identify any user-defined functions and global objects that the current function depends on. Append the hashes of those dependencies to the string representation of the current function.

4.  Compute the hash of the final string representation using `targets:::digest_chr64()`.

Above, (3) is important because user-defined functions have dependencies of their own, such as other user-defined functions and other global objects. (3) ensures that a change to a function's dependencies invalidates the function itself, which in turn invalidates any calling functions and any targets downstream with the `depend` cue turned on.

## 

**END** tar_cue() documentation

## 

If you change one of your functions, the targets that depend on it will no longer be up to date, and `tar_make()` will rebuild them. For example, let's set the number of bins in our histogram.

We would see similar behavior if we changed the R expressions in any `tar_target()` calls in the target script file.

\

**START** static and dynamic files info from the web <https://www.petersweb.me.uk/docs/what-is-the-difference-between-a-static-file-and-a-dynamic-file/>

in essence a static file is just that -- static i.e. stays as it is and doesn't change.  A dynamic file is capable of changing its output based on what is being inputed into the file.

Static content is content that does not change, every time a static file is downloaded it will always remain the same. As a result a static file is much easier to cache than a dynamic file and uses up much less resources when it is being accessed by a visitor.

These files remain the same once produced and very rarely change

Dynamic content changes based on input, a good comparison would be to compare dynamic content to algebraic subjects -- lets say I wrote a webpage that functions as a basic calculator, I would need to take input (a) and input (b) then I would need to write a script to say "tell me what (a) plus (b) amounts to.." (of course this is highly simplified). I can't cache the page as I don't know what the values of (a) and (b) are...as you can probably guess this would take up a lot more resources than a static file and put more stress on a server.  Although this makes dynamic files/content sound like a drawback they are an essential component of a modern website, especially a website that needs to relay a stream of constantly changing information back to a user (think travel websites, booking a flight or a train etc). A lot of websites such as this use dynamic content that is often stored in a database (or in a similar format), a simple example of this is our Knowledgebase section here at Peters Web, one file is responsible for generating all of the articles at any one time. This illustrates how useful a dynamic file can be as although it commands more processing effort/time, only one file is required to fetch any of the articles requested from our Knowledgebase. 

 Although dynamic content is extremely useful it is harder to cache than static content due to its changeable nature, however it is worth noting that in the right context caching is possible. Going back to the calculator example, lets say a user enters the sum '1+1' we could then run the script and cache the output (which is 2 just in case you were wondering.....). This is possible as '1+1' is always going to equal 2 thus nothing needs to be recalculated once you have arrived at this answer. 

In [computing](https://en.wikipedia.org/wiki/Computing "Computing"), a **cache** is a hardware or software component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. A *cache hit* occurs when the requested data can be found in a cache, while a *cache miss* occurs when it cannot. Cache hits are served by reading data from the cache, which is faster than recomputing a result or reading from a slower data store; thus, the more requests that can be served from the cache, the faster the system performs.

**END** static dynamic files info from the web.

\
If we change the data file `raw_data.csv`, `targets` notices the change. This is because `raw_data_file` is a dynamic file (i.e. `tar_target(format = "file")`) that returned `"raw_data.csv"`.

`targets` has a convenient functions `tar_read()` to read your data from the `_targets/` data store.

    tar_read(hist)

There is also a `tar_load()` function, which loads the return values of targets into the current environment (or the environment of your choosing) and returns nothing (`NULL`). `tar_load()` supports [`tidyselect`](https://tidyselect.r-lib.org/) verbs like `starts_with().`

The purpose of `tar_read()` and `tar_load()` is to make exploratory data analysis easy and convenient. Use these functions to verify the correctness of the output from the pipeline and come up with ideas for new targets if needed.

To read the build progress of your targets while `tar_make()` is running, you can open a new R session and run `tar_progress()`. It reads the spreadsheet in `_targets/meta/progress` and tells you which targets are running, built, errored, or cancelled.

Likewise, the `tar_meta()` function reads `_targets/meta/meta` and tells you high-level information about the target's settings, data, and results. The `warnings`, `error`, and `traceback` columns give you diagnostic information about targets with problems.

The `_targets/meta/meta` spreadsheet file is critically important. Although `targets` can still work properly if files are missing from `_targets/objects`, the pipeline will error out if `_targets/meta/meta` is corrupted. If `tar_meta()` works, the project should be fine.

\

------------------------------------------------------------------------

3\. TARGET MARKDOWN

Target Markdown, available in `targets` \> 0.6.0, is a powerful R Markdown interface for reproducible analysis pipelines. With Target Markdown, you can define a fully scalable pipeline from within one or more R Markdown reports, anything from a single report to a whole `bookdown` or `workflowr` project. You get the best of both worlds: the human readable narrative of literate programming, and the sophisticated caching and dependency management systems of `targets`.

Target Markdown has two primary objectives:

1.  Interactively explore, prototype, and test the components of a `targets` pipeline using the R Markdown [notebook interface](https://bookdown.org/yihui/rmarkdown/notebook.html).

2.  Set up a `targets` pipeline using convenient R Markdown code chunks.

Target Markdown supports a special `{targets}` [language engine](https://bookdown.org/yihui/rmarkdown-cookbook/other-languages.html) with an interactive mode for (1) and a non-interactive mode for (2). By default, the mode is interactive in the [notebook interface](https://bookdown.org/yihui/rmarkdown/notebook.html) and non-interactive when you knit/render the whole document.^[4](https://books.ropensci.org/targets/markdown.html#fn4)^. You can set the mode using the `tar_interactive` chunk option.

**CHECK** targets-minimal for an example on a target markdown.

First, load `targets` to activate the specialized `knitr` engine for Target Markdown.

    ```{r}
    library(targets)
    ```

Non-interactive Target Markdown writes scripts to a special `_targets_r/` directory to define individual targets and global objects. In order to keep your target definitions up to date, it is recommended to remove `_targets_r/` at the beginning of the R Markdown document(s) in order to clear out superfluous targets and globals from a previous version. `tar_unscript()` is a convenient way to do this.

    ```{r}
    tar_unscript()
    ```

\
As usual, your targets depend on custom functions, global objects, and `tar_option_set()` options you define before the pipeline begins. Define these globals using the `{targets}` engine with `tar_globals = TRUE` chunk option.

    ```{targets some-globals, tar_globals = TRUE, tar_interactive = TRUE}
    options(tidyverse.quiet = TRUE)
    tar_option_set(packages = c("biglm", "dplyr", "ggplot2", "readr", "tidyr"))
    create_plot <- function(data) {
      ggplot(data) +
        geom_histogram(aes(x = Ozone), bins = 12) +
        theme_gray(24)
    }
    ```

In interactive mode, the chunk simply runs the R code in the `tar_option_get("envir")` environment (usually the global environment) and displays a message:

    #> Run code and assign objects to the environment.

Here is the same chunk in non-interactive mode. Normally, there is no need to duplicate chunks like this, but we do so here in order to demonstrate both modes.

    ```{targets chunk-name, tar_globals = TRUE, tar_interactive = FALSE}
    options(tidyverse.quiet = TRUE)
    tar_option_set(packages = c("biglm", "dplyr", "ggplot2", "readr", "tidyr"))
    create_plot <- function(data) {
      ggplot(data) +
        geom_histogram(aes(x = Ozone), bins = 12) +
        theme_gray(24)
    }
    ```

In non-interactive mode, the chunk establishes a common `_targets.R` file and writes the R code to a script in `_targets_r/globals/`, and displays an informative message:^[5](https://books.ropensci.org/targets/markdown.html#fn5)^

    #> Establish _targets.R and _targets_r/globals/chunk-name.R.

It is good practice to assign explicit chunk labels or set the `tar_name` chunk option on a chunk-by-chunk basis. Each chunk writes code to a script path that depends on the name, and all script paths need to be unique.^[6](https://books.ropensci.org/targets/markdown.html#fn6)^

To define targets of the pipeline, use the `{targets}` language engine with the `tar_globals` chunk option equal `FALSE` or `NULL` (default). The return value of the chunk must be a target object or a list of target objects, created by `tar_target()` or a similar function.

Below, we define a target to establish the air quality dataset in the pipeline.

    ```{targets raw-data, tar_interactive = TRUE}
    tar_target(raw_data, airquality)
    ```

If you run this chunk in interactive mode, the target's R command runs, the engine tests if the output can be saved and loaded from disk correctly, and then the return value gets assigned to the `tar_option_get("envir")` environment (usually the global environment).

    #> Run targets and assign them to the environment.

In the process, some temporary files are created and destroyed, but your local file space will remain untouched (barring any custom side effects in your custom code).

After you run a target in interactive mode, the return value is available in memory, and you can write an ordinary R code chunk to read it.

    ```{r}
    head(raw_data)
    ```

The output is the same as what `tar_read(raw_data)` would show after a serious pipeline run.

For demonstration purposes, here is the `raw_data` target code chunk in non-interactive mode.

    ```{targets chunk-name-with-target, tar_interactive = FALSE}
    tar_target(raw_data, airquality)
    ```

In non-interactive mode, the `{targets}` engine does not actually run any targets. Instead, it establishes a common `_targets.R` and writes the code to a script in `_targets_r/targets/`.

    #> Establish _targets.R and _targets_r/targets/chunk-name-with-target.R.

Next, we define more targets to process the raw data and plot a histogram. Only the returned value of the chunk code actually becomes part of the pipeline, so if you define multiple targets in a single chunk, be sure to wrap them all in a list.

    ```{targets downstream-targets}
    list(
      tar_target(data, raw_data %>% filter(!is.na(Ozone))),
      tar_target(hist, create_plot(data))
    )
    ```

In non-interactive mode, the whole target list gets written to a single script.

    #> Establish _targets.R and _targets_r/targets/downstream-targets.R.

If you ran all the `{targets}` chunks in non-interactive mode (i.e. pipeline construction mode), then the target script file and helper scripts should all be established, and you are ready to run the pipeline in with `tar_make()` in an ordinary `{r}` code chunk. This time, the output is written to persistent storage at the project root.

You can retrieve results from the `_targets/` data store using `tar_read()` or `tar_load()`.

    ```{r}
    library(biglm)
    tar_read(fit)
    ```

The `targets` dependency graph helps your readers understand the steps of your pipeline at a high level.

    ```{r}
    tar_visnetwork()
    ```

At this point, you can go back and run `{targets}` chunks in interactive mode without interfering with the code or data of the non-interactive pipeline.

\

`targets` version 0.6.0.9001 and above supports the `tar_interactive()` function, which suppresses code unless Target Markdown interactive mode is turned on. Similarly, `tar_noninteractive()` suppresses code in interactive mode, and `tar_toggle()` selects alternative pieces of code based on the current mode.

\
....

## 3.11 Chunk options

-   `tar_globals`: Logical of length 1, whether to define globals or targets. If `TRUE`, the chunk code defines functions, objects, and options common to all the targets. If `FALSE` or `NULL` (default), then the chunk returns formal targets for the pipeline.

-   `tar_interactive`: Logical of length 1 to choose whether to run the chunk in interactive mode or non-interactive mode.

-   `tar_name`: name to use for writing helper script files (e.g. \_targets_r/targets/target_script.R) and specifying target names if the tar_simple chunk option is TRUE. All helper scripts and target names must have unique names, so please do not set this option globally with knitr::opts_chunk\$set().

-   `tar_script`: Character of length 1, where to write the target script file in non-interactive mode. Most users can skip this option and stick with the default `_targets.R` script path. Helper script files are always written next to the target script in a folder with an `"_r"` suffix. The `tar_script` path must either be absolute or be relative to the project root (where you call `tar_make()` or similar). If not specified, the target script path defaults to `tar_config_get("script")` (default: `_targets.R`; helpers default: `_targets_r/`). When you run `tar_make()` etc. with a non-default target script, you must select the correct target script file either with the `script` argument or with `tar_config_set(script = ...)`. The function will `source()` the script file from the current working directory (i.e. with `chdir = FALSE` in `source()`).

-   `tar_simple`: Logical of length 1. Set to `TRUE` to define a single target with a simplified interface. In code chunks with `tar_simple` equal to `TRUE`, the chunk label (or the `tar_name` chunk option if you set it) becomes the name, and the chunk code becomes the command. In other words, a code chunk with label `targetname` and command `mycommand()` automatically gets converted to `tar_target(name = targetname, command = mycommand())`. All other arguments of `tar_target()` remain at their default values (configurable with `tar_option_set()` in a `tar_globals = TRUE` chunk).

------------------------------------------------------------------------

4\. DEBUGGING.

This chapter explains how to monitor the progress of your pipeline and troubleshoot performance issues.

If you are using `targets`, then you probably have an intense computation like Bayesian data analysis or machine learning. These tasks take a long time to run, and it is a good idea to monitor them. Here are some options built directly into `targets`:

1.  `tar_poll()` continuously refreshes a text summary of runtime progress in the R console. Run it in a new R session at the project root directory. (Only supported in `targets` version 0.3.1.9000 and higher.)

2.  `tar_visnetwork()`, `tar_progress_summary()`, `tar_progress_branches()`, and `tar_progress()` show runtime information at a single moment in time.

3.  `tar_watch()` launches an Shiny app that automatically refreshes the graph every few seconds. Try it out in the example below.

<!-- -->

    # Define an example target script file with a slow pipeline.
    library(targets)
    tar_script({
      sleep_run <- function(...) {
        Sys.sleep(10)
      }
      list(
        tar_target(settings, sleep_run()),
        tar_target(data1, sleep_run(settings)),
        tar_target(data2, sleep_run(settings)),
        tar_target(data3, sleep_run(settings)),
        tar_target(model1, sleep_run(data1)),
        tar_target(model2, sleep_run(data2)),
        tar_target(model3, sleep_run(data3)),
        tar_target(figure1, sleep_run(model1)),
        tar_target(figure2, sleep_run(model2)),
        tar_target(figure3, sleep_run(model3)),
        tar_target(conclusions, sleep_run(c(figure1, figure2, figure3)))
      )
    })

    # Launch the app in a background process.
    # You may need to refresh the browser if the app is slow to start.
    # The graph automatically refreshes every 10 seconds
    tar_watch(seconds = 10, outdated = FALSE, targets_only = TRUE)

    # Now run the pipeline and watch the graph change.
    px <- tar_make()

`tar_watch_ui()` and `tar_watch_server()` make this functionality available to other apps through a Shiny module.

Unfortunately, none of these options can tell you if any [parallel workers](https://books.ropensci.org/targets/hpc.html#hpc) or external processes are still running. You can monitor local processes with a utility like `top` or `htop`, and traditional HPC scheduler like SLURM or SGE support their own polling utilities such as `squeue` and `qstat`. `tar_process()` and `tar_pid()` get the process ID of the main R process that last attempted to run the pipeline.

If your pipeline has several thousand targets, functions like `tar_make()`, `tar_outdated()`, and `tar_visnetwork()` may take longer to run. There is an inevitable per-target runtime cost because package needs to check the code and data of each target individually. If this overhead becomes too much, consider batching your work into a smaller group of heavier targets. Using your custom functions, you can make each target perform multiple iterations of a task that was previously given to targets one at a time. For details and an example, please see the discussion on batching at the bottom of the [dynamic branching chapter](https://books.ropensci.org/targets/dynamic.html#dynamic).

With dynamic branching, it is super easy to create an enormous number of targets. But when the number of targets starts to exceed a couple hundred, `tar_make()` slows down, and graphs from `tar_visnetwork()` start to become unmanageable.

In `targets` version 0.5.0.9000, the `names` and `shortcut` arguments to `tar_make()` provide an alternative workaround. `tar_make(names = all_of("only", "these", "targets"), shortcut = TRUE)` can completely omit thousands of upstream targets for the sake of concentrating on one section at a time. However, this technique is only a temporary measure, and it is best to eventually revert back to the default `names = NULL` and `shortcut = FALSE` to ensure reproducibility.

In the case of dynamic branching, another temporary workaround is to temporarily select subsets of branches. For example, instead of `pattern = map(large_target)` in `tar_target()`, you could prototype on a target that uses `pattern = head(map(large_target), n = 1)` or `pattern = slice(map(large_target), c(4, 5, 6))`. In the case of `slice()`, the `tar_branch_index()` function (only in `targets` version 0.5.0.9000 and above) can help you find the required integer indexes corresponding to individual branch names you may want.

------------------------------------------------------------------------

6\. FUNCTIONS

[`targets`](https://github.com/ropensci/targets) expects users to adopt a function-oriented style of programming. User-defined R functions are essential to express the complexities of data generation, analysis, and reporting. This chapter explains what makes functions useful and how to leverage them in your pipelines.

\
Functions are the building blocks of most computer code. They make code easier to think about, and they break down complicated ideas into small manageable pieces. Out of context, you can develop and test a function in isolation without mentally juggling the rest of the project. In the context of the whole workflow, functions are convenient shorthand to make your work easier to read.

In addition, functions are a nice mental model to express data science. A data analysis workflow is a sequence of transformations: datasets map to analyses, and analyses map to summaries. In fact, a function for data science typically falls into one of three categories:

1.  Process a dataset.

2.  Analyze a dataset.

3.  Summarize an analysis.

\
\
Now, instead of invoking a whole block of text, all you need to do is type a small reusable command. The function name speaks for itself, so you can recall what it does without having to mentally process all the details again.

Without those functions, our pipeline in the [walkthrough chapter](https://books.ropensci.org/targets/walkthrough.html#walkthrough) would look long, complicated, and difficult to digest.

    # _targets.R
    library(targets)
    source("R/functions.R")
    options(tidyverse.quiet = TRUE)
    tar_option_set(packages = c("biglm", "rmarkdown", "tidyverse"))
    list(
      tar_target(raw_data_file, "data/raw_data.csv", format = "file"),
      tar_target(raw_data, read_csv(raw_data_file, col_types = cols())),
      tar_target(
        data,
        raw_data %>%
          filter(!is.na(Ozone))
      ),
      tar_target(fit, biglm(Ozone ~ Wind + Temp, data)),
      tar_target(
        hist,
        ggplot(data) +
          geom_histogram(aes(x = Ozone)) +
          theme_gray(24)
      )
    )

But if we write our functions in `R/functions.R` and `source()` them into the target script file (default: `_targets.R`) the pipeline becomes much easier to read. We can even condense out `raw_data` and `data` targets together without creating a large command.

    # _targets.R
    library(targets)
    source("R/functions.R")
    options(tidyverse.quiet = TRUE)
    tar_option_set(packages = c("biglm", "tidyverse"))
    list(
      tar_target(raw_data_file, "data/raw_data.csv",format = "file"),
      tar_target(data, read_and_clean(raw_data_file)),
      tar_target(fit, fit_model(data)),
      tar_target(hist, create_plot(data))
    )

\

------------------------------------------------------------------------

7\. TARGET CONSTRUCTION

Targets are high-level steps of the workflow that run the work you define in your functions. A target runs some R code and saves the returned R object to storage, usually a single file inside `_targets/objects/`.

A target is an abstraction. The `targets` package automatically manages data storage and retrieval under the hood, which means you do not need to reference a target's data file directly (e.g. `_targets/objects/your_target_name`). Instead, your R code should refer to a target name as if it were a variable in an R session. In other words, from the point of view of the user, a target is an R object in memory. That means a target name must be a valid visible symbol name for an R variable. The name must not begin with a dot, and it must be a string that lets you assign a value, e.g. `your_target_name <- TRUE`

Like a good function, a good target generally does one of three things:

1.  Create a dataset.

2.  Analyze a dataset with a model.

3.  Summarize an analysis or dataset.

If a function gets too long, you can split it into nested sub-functions that make your larger function easier to read and maintain.

The `targets` package automatically skips targets that are already up to date, so it is best to define targets that maximize time savings. Good targets usually

1.  Are large enough to subtract a decent amount of runtime when skipped.

2.  Are small enough that some targets can be skipped even if others need to run.

3.  Invoke no side effects such as modifications to the global environment. (But targets with `tar_target(format = "file")` can save files.)

4.  Return a single value that is

    1.  Easy to understand and introspect.

    2.  Meaningful to the project.

    3.  Easy to save as a file, e.g. with `readRDS()`. Please avoid [non-exportable objects](https://cran.r-project.org/web/packages/future/vignettes/future-4-non-exportable-objects.html) as target return values or global variables.

Regarding the last point above, it is possible to customize the storage format of the target. For details, enter `?tar_target` in the console and scroll down to the description of the `format` argument.

Each target runs R code, so to invoke a tool outside R, consider `system2()` or [`processx`](https://processx.r-lib.org/) to call the appropriate system commands. This technique allows you to run shell scripts, Python scripts, etc. from within R. External scripts should ideally be tracked as input files using `tar_target(format = "file")` as described in [section on external input files](https://books.ropensci.org/targets/files.html#files). There are also specialized R packages to retrieve data from remote sources and invoke web APIs, including [`rnoaa`](https://github.com/ropensci/rnoaa), [`ots`](https://github.com/ropensci/ots), and [`aws.s3`](https://github.com/cloudyr/aws.s3), and you may wish to use [custom cues](https://docs.ropensci.org/targets/reference/tar_cue.html) to automatically invalidate a target when the upstream remote data changes.

Like a good pure function, a good target should return a single value and not produce side effects. (The exception is [output file targets](https://books.ropensci.org/targets/files.html#files) which create files and return their paths.) Avoid modifying the global environment with calls to `data()` or `source()`. If you need to source scripts to define global objects, please do so at the top of your target script file (default: `_targets.R`) just like `source("R/functions.R")` from the [walkthrough vignette](https://books.ropensci.org/targets/walkthrough.html#walkthrough).

<!--# In computer science, an operation, function or expression is said to have a side effect if it modifies some state variable value(s) outside its local environment, that is to say has an observable effect besides returning a value (the intended effect) to the invoker of the operation. State data updated "outside" of the operation may be maintained "inside" a stateful object or a wider stateful system within which the operation is performed. Example side effects include modifying a non-local variable, modifying a static local variable, modifying a mutable argument passed by reference, performing I/O or calling other side-effect functions.[1] In the presence of side effects, a program's behaviour may depend on history; that is, the order of evaluation matters. Understanding and debugging a function with side effects requires knowledge about the context and its possible histories.[2][3]                                                          To understand functional programming first, we need to understand side effects. A side effect is when a function relies on, or modifies, something outside its parameters to do something. For example, a function which reads or writes from a variable outside its own arguments, a database, a file, or the console can be described as having side effects. A function is pure if, given the same inputs, it a) always returns the same output and b) does not have any side effects. -->

`targets` automatically loads dependencies into memory when they are required, so it is rarely advisable to call `tar_read()` or `tar_load()` from inside a target. Except in rare circumstances, `tar_read()` and `tar_load()` are only for exploratory data analysis and [literate programming](https://books.ropensci.org/targets/files.html#files).

The return value of a target should be an R object that can be saved to disk and hashed.

The object should be compatible with the storage format you choose using the `format` argument of `tar_target()` or `tar_option_set()`. For example, if the format is `"rds"` (default), then the target should return an R object that can be saved with `saveRDS()` and safely loaded properly into another session. Please avoid returning [non-exportable objects](https://cran.r-project.org/web/packages/future/vignettes/future-4-non-exportable-objects.html) such as connection objects, `Rcpp` pointers, `xgboost` matrices, and `greta` models^[11](https://books.ropensci.org/targets/targets.html#fn11)^.

\

Once a target is saved to disk, `targets` computes a [`digest`](https://eddelbuettel.github.io/digest/) hash to track changes to the data file(s). These hashes are used to decide whether each target is up to date or needs to rerun. In order for the hash to be useful, the data you return from a target must be an accurate reflection of the underlying content of the data. So please try to return the actual data instead of an object that wraps or points to the data. Otherwise, the package will make incorrect decisions regarding which targets can skip and which need to rerun.

------------------------------------------------------------------------

8.PACKAGES

For most pipelines, it is straightforward to load the R packages that your targets need in order to run. You can either:

1.  Call `library()` at the top of the target script file (default: `_targets.R`) to load each package the conventional way, or

2.  Name the required packages using the `packages` argument of `tar_option_set()`.

2\. is often faster, especially for utilities like `tar_visnetwork()`, because it avoids loading packages unless absolutely necessary.

Some package management workflows are more complicated. If your use special configuration with [conflicted](https://github.com/r-lib/conflicted), [`box`](https://klmr.me/box/), [`import`](https://import.rticulate.org/), or similar utility, please do your configuration inside a project-level `.Rprofile` file instead of the target script file (default: `_targets.R`). In addition, if you use distributed workers inside external containers (**Docker**, Singularity, AWS AMI, etc.) make sure each container has a copy of this same `.Rprofile` file where the R worker process spawns. This approach is ensures that all [remote workers](https://books.ropensci.org/targets/hpc.html#hpc) are configured the same way as the local main process.

------------------------------------------------------------------------

9.PROJECTS

A project is a `targets` pipeline together with its supporting source code, data, and configuration settings. This chapter explains best practices when it comes to organizing and configuring `targets` projects.

For extra reproducibility, it is good practice to use the `renv` R package for package management and Git/GitHub for code version control. The entire `_targets/` data store should generally not be committed to Git because of its large size.^[12](https://books.ropensci.org/targets/projects.html#fn12)^ The broader R community has excellent resources and tutorials on getting started with these third-party tools.

`targets` is mostly indifferent to how you organize the files in your project. However, it is good practice to follow the overall structure of a research compendium or R package (not necessarily with a `DESCRIPTION` file). It also is good practice to give each project its own unique folder with one `targets` pipeline, one `renv` library for package management, and one Git/GitHub repository for code version control. As described later, it is possible to create multiple overlapping projects within a single folder, but this is not recommended for most situations.

The [walkthrough chapter](https://books.ropensci.org/targets/walkthrough.html#walkthrough) shows the file structure for a minimal `targets` project. For more serious projects, the file system may expand to look something like this:

├── .git/

├── .Rprofile

├── .Renviron

├── renv/

├── index.Rmd

├── \_targets/

├── \_targets.R

├── \_targets.yaml

├── R/

├──── functions_data.R

├──── functions_analysis.R

├──── functions_visualization.R

├── data/

└──── input_data.csv

Some of these files are optional, and they have the following roles.

-   `.git/`: a folder automatically created by Git for version control purposes.

-   `.Rprofile`: a text file automatically created by `renv` to automatically load the project library when you start R at the project root folder. You may wish to add other global configuration here, e.g. declare package precedence using the `conflicted` package.

-   `.Renviron`: a text file of key-value pairs defining project-level environment variables, e.g. API keys and package settings. See `Sys.getenv()` for more information on environment variables and how to work with them in R.\

-   `index.Rmd`: [Target Markdown](https://books.ropensci.org/targets/markdown.html#markdown) report source file to define the pipeline.

-   `_targets/`: the data store where `tar_make()` and similar functions write target storage and metadata when they run the pipeline.

-   `_targets.R`: the [target script file](https://docs.ropensci.org/targets/reference/tar_script.html). All `targets` pipelines must have a target script file that returns a target list at the end. If you use [Target Markdown](https://books.ropensci.org/targets/markdown.html#markdown) (e.g. `index.Rmd` above) then the target script will be written automatically. Otherwise, you may write it by hand. Unless you apply the custom configuration described later in this chapter, the target script file will always be called `_targets.R` and live at the project root folder.

-   `_targets.yaml`: a YAML file to set default arguments to critical functions like `tar_make()`. As described below, you can access and modify this file with functions `tar_config_get()`, `tar_config_set()`, and `tar_config_unset()`. `targets` will attempt to look for `_targets.yaml` unless you set a different path in the `TAR_CONFIG` environment variable.

-   `R/`: directory of scripts containing custom user-defined R code. Most of the code will likely contain [custom functions](https://books.ropensci.org/targets/functions.html#functions) you write to support your targets. You can load these functions with `source("R/function_script.R")` or `eval(parse(text = "R/function_script.R")`, either in a `tar_globals = TRUE` code chunk in [Target Markdown](https://books.ropensci.org/targets/markdown.html#markdown) or directly in `_targets.R` if you are not using [Target Markdown](https://books.ropensci.org/targets/markdown.html#markdown).

-   `data/`: directory of local input data files. As described in the [files chapter](https://books.ropensci.org/targets/files.html#files), it is good practice to track input files using `format = "file"` in `tar_target()` and then reference those file targets in downstream targets that directly depend on those files.

It is generally good practice to give each project its own unique folder with one `targets` pipeline, one `renv` library for package management, and one Git/GitHub repository for code version control. However, sometimes it is reasonable to maintain multiple pipelines within a project: for example, if different pipelines have similar research goals and share the same code base of custom user-defined functions. This section explains how to maintain and navigate such a collection of overlapping projects.

The functionality below assumes you have `targets` version 0.7.0.9001 or higher, which you may need to install from GitHub.

    remotes::install_github("ropensci/targets")

.....................................

------------------------------------------------------------------------

10\. EXTERNAL FILES AND LITERATE PROGRAMMING.

Each project has a local data store. This is usually a special `_targets/` folder at the project root directory (where you call `tar_make()`). In `targets` version 0.3.1.9000 and above, you can set the location to something other than `_targets/` using [`tar_config_set()`](https://docs.ropensci.org/targets/reference/tar_config_set.html), which writes to a project-level `_targets.yaml` file at the project root. The files in the data store are structured as follows.

    _targets/ # Can be customized with tar_config_set().
    ├── meta/
    ├────── meta
    ├────── process
    ├────── progress
    ├── objects/
    ├────── target1 
    ├────── target2
    ├────── branching_target_c7bcb4bd
    ├────── branching_target_285fb6a9
    ├────── branching_target_874ca381
    └── scratch/ # tar_make() deletes this folder after it finishes.

Spreadsheets `_targets/meta/meta` keeps track of target metadata, `_targets/meta/progress` records runtime progress, and `_targets/meta/process` has high-level information (such as process ID) about the external R session orchestrating the targets. The `scratch/` directory contains temporary files which can be safely deleted after `tar_make()` finishes. The `_targets/objects/` folder contains the return values of the targets themselves.

A typical target returns an R object: for example, a dataset with `tar_target(dataset, data.frame(x = rnorm(1000)), format = "fst")` or a fitted model `tar_target(model, biglm(ozone ~ temp + wind), format = "qs")`. When you run the pipeline, `targets` computes this object and saves it as a file in `_targets/objects/`. The file name in `_targets/objects/` is always the target name, and type of the file is determined by the `format` argument of `tar_target()`, and formats `"fst"` and `"qs"` are two of many choices explained in the help file of `tar_target()`. No matter what format you pick, `targets` watches the file for changes and recomputes the target in `tar_make()` if the the file gets corrupted (unless you suppress the file cue with `tar_target(cue = tar_cue(file = FALSE))`).

There are [multiple functions](https://docs.ropensci.org/targets/reference/index.html#section-clean) to remove target storage or modify the metadata to force one or more targets to rerun in the next call to `tar_make()`.

-   [`tar_destroy()`](https://docs.ropensci.org/targets/reference/tar_destroy.html) is by far the most commonly used cleaning function. It removes the `_targets/` data store completely, deleting all the results from [`tar_make()`](https://docs.ropensci.org/targets/reference/tar_make.html) except for external files. Use it if you intend to start the pipeline from scratch without any trace of a previous run.

-   [`tar_prune()`](https://docs.ropensci.org/targets/reference/tar_prune.html) deletes the data and metadata of all the targets no longer present in your current target script file (default: `_targets.R`). This is useful if you recently worked through multiple changes to your project and are now trying to discard irrelevant data while keeping the results that still matter.

-   [`tar_delete()`](https://docs.ropensci.org/targets/reference/tar_delete.html) is more selective than [`tar_destroy()`](https://docs.ropensci.org/targets/reference/tar_destroy.html) and [`tar_prune()`](https://docs.ropensci.org/targets/reference/tar_prune.html). It removes the individual data files of a given set of targets from `_targets/objects/` while leaving the metadata in `_targets/meta/meta` alone. If you have a small number of data-heavy targets you need to discard to conserve storage, this function can help.

-   [`tar_invalidate()`](https://docs.ropensci.org/targets/reference/tar_invalidate.html) is the opposite of [`tar_delete()`](https://docs.ropensci.org/targets/reference/tar_delete.html): for the selected targets, it deletes the metadata in `_targets/meta/meta` but keeps the return values in `_targets/objects/`. After invalidation, you will still be able to locate the data files with [`tar_path()`](https://docs.ropensci.org/targets/reference/tar_path.html) and manually salvage them in an emergency. However, [`tar_load()`](https://docs.ropensci.org/targets/reference/tar_load.html) and [`tar_read()`](https://docs.ropensci.org/targets/reference/tar_read.html) will not be able to read the data into R, and subsequent calls to [`tar_make()`](https://docs.ropensci.org/targets/reference/tar_make.html) will attempt to rebuild those targets.

Target Markdown automatically supplies a `_targets.R` file and supporting scripts in a folder called `_targets_r/`. These paths can be customized together the `script` argument of [`tar_config_set()`](https://docs.ropensci.org/targets/reference/tar_config_set.html).

    _targets_r/ # Can be customized with tar_config_set(script = "...").
    ├── globals/ # Code chunks to define globals and settings (tar_globals = TRUE)
    ├────── globals_chunk1.R # Names follow chunk names or the tar_name chunk opt.
    ├────── globals_chunk2.R
    ├────── ...
    ├── targets/ # Code chunks to define groups of one or more targets
    ├────── targets_chunk1.R
    ├────── targets_chunk2.R
    └────── ...

Over time, the number of script files in `_targets_r/` starts to build up, and `targets` has no way of automatically removing helper scripts files that are no longer necessary. To keep your pipeline up to date with the code chunks in the Target Markdown document(s), it is good practice to call [`tar_unscript()`](https://docs.ropensci.org/targets/reference/tar_unscript.html) at the beginning of your first Target Markdown document. That way, extraneous/discarded targets are automatically removed from the pipeline when the document starts render.

To reproducibly track an external input file, you need to define a new target that has

1.  A command that returns the file path as a character vector, and

2.  `format = "file"` in `tar_target()`.

When the target runs in the pipeline, the returned character vector gets recorded in `_targets/meta`, and `targets` watches the data file and invalidates the target when that file changes. To track multiple files this way, simply define a multi-element character vector where each element is a path. Each element can also be a directory, but this directory must not be empty at the time the target runs.

The first two targets of the [minimal example](https://github.com/ropensci/targets-minimal) demonstrate how to track an input file.

    # _targets.R
    library(targets)
    path_to_data <- function() {
      "data/raw_data.csv"
    }
    list(
      tar_target(
        raw_data_file,
        path_to_data(),
        format = "file"
      ),
      tar_target(
        raw_data,
        read_csv(raw_data_file, col_types = cols())
      )
    )

Above, `raw_data_file` is the dynamic file target. The file `data/raw_data.csv` exists before we ever run the pipeline, and the R expression for the target returns the character vector `"data/raw_data.csv"`. (We use the `path_to_data()` function to demonstrate that you need not literally write `"data/raw_data.csv"` as long as the path is returned somehow.)

All subsequent targets that depend on the file must reference the file using the symbol `raw_data_file`. This allows `targets`' automatic static code analysis routines to detect which targets depend on the file. Because the `raw_data` target literally mentions the symbol `raw_data_file`, `targets` knows `raw_data` depends on `raw_data_file`. This ensures that

1.  `raw_data_file` gets processed before `raw_data`, and

2.  `tar_make()` automatically reruns `raw_data` if `raw_data_file` or `"data/raw_data.csv"` change.

If we were to omit the symbol `raw_data_file` from the R expression of `raw_data`, those targets would be disconnected in the graph and `tar_make()` would make incorrect decisions.

Output files have the same mechanics as input files. The target uses `format = "file"`, and the return value is a character value of paths to existing files and nonempty directories. The only difference here is that the target's R command writes to storage before it returns a value. For example, here is an output file target that saves a visualization.

    tar_target(
      plot_file,
      save_plot_and_return_path(),
      format = "file"
    )

Here, our custom `save_plot_and_return_path()` function does exactly what the name describes.

    save_plot_and_return_path <- function() {
      plot <- ggplot(mtcars) +
        geom_point(aes(x = wt, y = mpg))
      ggsave("plot_file.png", plot, width = 7, height = 7)
      return("plot_file.png")
    }

`targets` has two ways to handle literate programming. The first is [Target Markdown](https://books.ropensci.org/targets/markdown.html#markdown), which is covered in a [different chapter](https://books.ropensci.org/targets/markdown.html#markdown). In [Target Markdown](https://books.ropensci.org/targets/markdown.html#markdown), `targets` serves as a caching mechanism for projects that use R Markdown as the primary overarching workflow manager. The second way `targets` handles literate programming is to render lightweight dependency-aware R Markdown reports inside the individual targets of a pipeline. This second approach is covered in this section.

If you render an R Markdown report as part of a target, the report should be lightweight: mostly prose, minimal code, fast execution, and no output other than the rendered HTML/PDF document. In other words, R Markdown reports are just targets that document prior results. The bulk of the computation should have already happened upstream, and the most of the code chunks in the report itself should be terse calls to `tar_read()` and `tar_load()`.

The report from the [minimal example](https://github.com/ropensci/targets-minimal) looks like this:

Above, the report depends on targets `fit` and `hist`. The use of `tar_read()` and `tar_load()` allows us to run the report outside the pipeline. As long as `_targets/` folder has data on the required targets from a previous `tar_make()`, you can open the RStudio IDE, edit the report, and click the Knit button like you would for any other R Markdown report.

To connect the target with the pipeline, we define a special kind of target using `tar_render()` from the [`tarchetypes`](https://github.com/ropensci/tarchetypes) package instead of the usual `tar_target()`, which

1.  Finds all the `tar_load()`/`tar_read()` dependencies in the report and inserts them into the target's command. This enforces the proper dependency relationships. (`tar_load_raw()` and `tar_read_raw()` are ignored because those dependencies cannot be resolved with static code analysis.)

2.  Sets `format = "file"` (see `tar_target()`) so `targets` watches the files at the returned paths.

3.  Configures the target's command to return both the output report files and the input source file. All these file paths are relative paths so the project stays portable.

4.  Forces the report to run in the user's current working directory instead of the working directory of the report.

5.  Sets convenient default options such as `deployment = "main"` in `tar_target()` and `quiet = TRUE` in `rmarkdown::render()`.

The target definition looks like this.

    library(tarchetypes)
    target <- tar_render(report, "report.Rmd") # Just defines a target object.
    target$command$expr[[1]]
    #> tarchetypes::tar_render_run(path = "report.Rmd", args = list(input = "report.Rmd", 
    #>     knit_root_dir = getwd(), quiet = TRUE), deps = list(fit, 
    #>     hist))

Because symbols `fit` and `hist` appear in the command, `targets` knows that `report` depends on `fit` and `hist`. When we put the `report` target in the pipeline, these dependency relationships show up in the graph.

In this scenario, the pipeline renders your [parameterized R Markdown](https://rmarkdown.rstudio.com/developer_parameterized_reports.html) report one time using a single set of parameters. These parameters can be upstream targets, global objects, or fixed values. Simply pass a `params` argument to [`tarchetypes::tar_render()`](https://docs.ropensci.org/tarchetypes/reference/tar_render.html):

    # _targets.R
    library(targets)
    library(tarchetypes)
    list(
      tar_target(data, data.frame(x = seq_len(26), y = letters))
      tar_render(report, "report.Rmd", params = list(your_param = data))
    )

the `report` target will run:

    # R console
    rmarkdown::render("report.Rmd", params = list(your_param = your_target))

where `report.Rmd` has the following YAML front matter:

    ---
    title: report
    output_format: html_document
    params:
      your_param: "default value"
    ---

and the following code chunk:

    print(params$your_param)

See [these examples](https://docs.ropensci.org/tarchetypes/reference/tar_render.html#examples) for a demonstration.

In this scenario, you still have a single report, but you render it multiple times over multiple sets of [R Markdown parameters](https://rmarkdown.rstudio.com/developer_parameterized_reports.html). This time, use [`tarchetypes::tar_render_rep()`](https://docs.ropensci.org/tarchetypes/reference/tar_render_rep.html) and write code to reference or generate a grid of parameters with one row per rendered report and one column per parameter. Optionally, you can also include an `output_file` column to control the file paths of the generated reports, and you can set the number of batches to reduce the overhead that would otherwise ensue from creating a large number of targets...

...

------------------------------------------------------------------------

15. WHAT ABOUT DRAKE.

The [`targets`](https://github.com/ropensci/targets) takes a friendlier, more transparent, less mysterious approach to data management. Its data store is a visible `_targets` folder, and it contains far fewer files: a spreadsheet of metadata, a spreadsheet of target progress, and one informatively named data file for each target. It is much easier to understand the data management process, identify and diagnose problems, place projects under version control, and avoid consuming unnecessary storage resources. Sketch:

    _targets/
    ├── meta/
    ├───── meta
    ├───── process
    ├───── progress
    ├── objects/
    ├───── target_name_1
    ├───── target_name_2
    ├───── target_name_3
    ├───── ...
    └── scratch/ # Deleted when the pipeline finishes.

In [`targets`](https://github.com/ropensci/targets), the metadata management system only updates information on global objects when the pipeline actually runs. This makes it possible to understand which specific changes to your code could have invalided your targets. In large projects with long runtimes, this feature contributes significantly to reproducibility and peace of mind.

------------------------------------------------------------------------

RETICULATE THEORY
